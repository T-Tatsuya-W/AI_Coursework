{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AI coursework\n",
    "\n",
    "\n",
    "using the mush.csv dataset.\n",
    "we will do binary classification with the three algorithms\n",
    "\n",
    "- tree decision thing\n",
    "- logistic regression\n",
    "- k nearest thing\n",
    "\n",
    "\n",
    "## Used Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, auc,roc_curve\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import scipy.stats\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data set :  (8416, 23)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('./mush.csv')\n",
    "print(\"data set : \", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for confusion matrix generation.\n",
    "def eval_confusion(model, X_test, y_test, y_pred):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    matrix = plot_confusion_matrix(model, X_test, y_test, cmap = plt.cm.Purples)\n",
    "    matrix.ax_.set_title('Confusion Matrix', color=\"black\")\n",
    "    plt.xlabel('Predicted Label', color=\"black\")\n",
    "    plt.ylabel('True Label', color=\"black\")\n",
    "    plt.gcf().axes[0].tick_params(colors=\"black\")\n",
    "    plt.gcf().axes[1].tick_params(colors=\"black\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have imported the (extended) dataset into the Pandas Dataframe `dataset` we can do some EDA\n",
    "# Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edible count: 4488(53.326996197718636%)\n",
      "poison count: 3928(46.67300380228137%)\n"
     ]
    }
   ],
   "source": [
    "edible = dataset[dataset['edibility']=='EDIBLE'].shape[0]\n",
    "poison = dataset[dataset['edibility']=='POISONOUS'].shape[0]\n",
    "print(\"edible count: \"+str(edible) + \"(\" + str(edible/dataset.shape[0] * 100) + \"%)\")\n",
    "print(\"poison count: \"+str(poison) + \"(\" + str(poison/dataset.shape[0] * 100) + \"%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Frequency Counts for each attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor col in columns:\\n    #series = dataset[col].value_counts()\\n    #print(series)\\n    #print(\"null values = \" + str(dataset[col].isnull().sum()))\\n    #print(\"\\n\")\\n    \\n    sns.set_theme()\\n    plot = sns.countplot(x=dataset[col], order=dataset[col].value_counts(ascending=False).index, palette = \"PuBuGn_d\");\\n    \\n    \\n    abs_values = dataset[col].value_counts(ascending=False)\\n    lbls = [f\\'{p[0]}\\' for p in zip(abs_values)]\\n    plot.bar_label(container=plot.containers[0], labels=lbls)\\n    \\n    plot.set_xticklabels(plot.get_xticklabels(), rotation=40)\\n\\n    plt.show()\\n    if col == \"bruises?\":\\n        col = \"bruises\"\\n    \\n    # To save new copies of the attribute Bar charts\\n    # plot.figure.savefig(\"Graphs/FrequencyBar/\"+col+\".png\", bbox_inches=\\'tight\\')\\n'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = dataset.columns\n",
    "\"\"\"\n",
    "for col in columns:\n",
    "    #series = dataset[col].value_counts()\n",
    "    #print(series)\n",
    "    #print(\"null values = \" + str(dataset[col].isnull().sum()))\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    sns.set_theme()\n",
    "    plot = sns.countplot(x=dataset[col], order=dataset[col].value_counts(ascending=False).index, palette = \"PuBuGn_d\");\n",
    "    \n",
    "    \n",
    "    abs_values = dataset[col].value_counts(ascending=False)\n",
    "    lbls = [f'{p[0]}' for p in zip(abs_values)]\n",
    "    plot.bar_label(container=plot.containers[0], labels=lbls)\n",
    "    \n",
    "    plot.set_xticklabels(plot.get_xticklabels(), rotation=40)\n",
    "\n",
    "    plt.show()\n",
    "    if col == \"bruises?\":\n",
    "        col = \"bruises\"\n",
    "    \n",
    "    # To save new copies of the attribute Bar charts\n",
    "    # plot.figure.savefig(\"Graphs/FrequencyBar/\"+col+\".png\", bbox_inches='tight')\n",
    "\"\"\"  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redundant attribute\n",
    "If we go through, we can see that attribute #16 'veil-type' is a useless attribute since all $8416$ rows have the same value. So, we can remove this column since it will do nothing but take up time. Now, we are down to 22 columns (including the `edibility` column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(['veil-type'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data\n",
    "Here we have printed out the the sums of all the values in each column, as well as a count of the number of null values in each. \n",
    "Since the null values in attribute $#11$ are represented by the string `'?'`, they are not registered as a null value by pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "edibility                      0\n",
       "cap-shape                      0\n",
       "cap-surface                    0\n",
       "cap-color                      0\n",
       "bruises?                       0\n",
       "odor                           0\n",
       "gill-attachment                0\n",
       "gill-spacing                   0\n",
       "gill-size                      0\n",
       "gill-color                     0\n",
       "stalk-shape                    0\n",
       "stalk-root                  2480\n",
       "stalk-surface-above-ring       0\n",
       "stalk-surface-below-ring       0\n",
       "stalk-color-above-ring         0\n",
       "stalk-color-below-ring         0\n",
       "veil-color                     0\n",
       "ring-number                    0\n",
       "ring-type                      0\n",
       "spore-print-color              0\n",
       "population                     0\n",
       "habitat                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.replace(\"?\", np.NaN)\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the missing values have been correctly replaced with the Pandas `NaN` indicator, we can deal with thees datapoints how we see fit.\n",
    "\n",
    "\n",
    "Two options would be to either delete the column with the missing data (`stalk-root` attribute $#11$) or delete the rows with the missing data\n",
    "\n",
    "By deleting the Column with the missing data, we will still have $8416$ entries, and only $21$ columns to work with (including edibility)\n",
    "\n",
    "On the other hand, by deleting the Rows with the missing values, we will keep $22$ columns but will go down to $5936$ columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8416, 21)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To delete rows with missing data\n",
    "# dataset.dropna(axis=0)\n",
    "\n",
    "# To delete attribute #11\n",
    "dataset = dataset.dropna(axis=1)\n",
    "dataset.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can do some more EDA, here we are going to create cross tabluations of each attribute with the output.\n",
    "\n",
    "## Cross Tabulations and $\\chi ^2$ tests\n",
    "\n",
    "Additionally, we have calculated the $\\chi^2$-test for independence $p$ values for each attribute against the output edibility\n",
    "\n",
    "Reminder that the chi squared test is used for a hypothesis test. In this case, the Hypothesis $H_0$ will be \"attribute 1 and attribute 2 are independent\", when our $p$ values are lower than 0.05, we can be 95% sure that $H_0$ is false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                p value\n",
      "edibility                  0.000000e+00\n",
      "cap-shape                 3.187210e-103\n",
      "cap-surface                5.996665e-76\n",
      "cap-color                  1.168726e-90\n",
      "bruises?                   0.000000e+00\n",
      "odor                       0.000000e+00\n",
      "gill-attachment            5.401073e-26\n",
      "gill-spacing              4.427957e-271\n",
      "gill-size                  0.000000e+00\n",
      "gill-color                 0.000000e+00\n",
      "stalk-shape                3.161053e-29\n",
      "stalk-surface-above-ring   0.000000e+00\n",
      "stalk-surface-below-ring   0.000000e+00\n",
      "stalk-color-above-ring     0.000000e+00\n",
      "stalk-color-below-ring     0.000000e+00\n",
      "veil-color                 6.088534e-39\n",
      "ring-number                2.630475e-79\n",
      "ring-type                  0.000000e+00\n",
      "spore-print-color          0.000000e+00\n",
      "population                 0.000000e+00\n",
      "habitat                    0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chisquaredvalues = pd.DataFrame(np.zeros((dataset.shape[1], 1)), columns = ['p value'], index = dataset.columns)\n",
    "min = 10\n",
    "max = 0\n",
    "\n",
    "for col1 in dataset.columns:\n",
    "    col2 = 'edibility'\n",
    "    crosstab = pd.crosstab(dataset[col2], dataset[col1])\n",
    "    chi2, p, dof, ex = chi2_contingency(crosstab)\n",
    "    chisquaredvalues.at[col1, 'p value'] =  p\n",
    "\n",
    "    if p>max:\n",
    "        max = p\n",
    "    if p<min:\n",
    "        min = p\n",
    "\n",
    "    \"\"\"\n",
    "    sns.set_theme()\n",
    "    # generate cross tabulations with edibility\n",
    "    heatplot = sns.heatmap(crosstab, annot = True, fmt = \"d\")\n",
    "    heatplot.set_xticklabels(heatplot.get_xticklabels(), rotation=45)\n",
    "    heatplot.set_yticklabels(heatplot.get_yticklabels(), rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # To save new copies of the crosstab heatmap\n",
    "    if col1 == \"bruises?\":\n",
    "        label = \"bruises\"\n",
    "    else:\n",
    "        label = col1\n",
    "    heatplot.figure.savefig(\"Graphs/CrossTabulations/\"+label+\"x\"+col2+\".png\", bbox_inches='tight')\n",
    " \"\"\"   \n",
    "    \n",
    "print(chisquaredvalues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the $p$ values for every attribute remaining is less than less than $0.05$, with the highest value being $5.4e-26$, we can see that there is enough sufficient evidence to reject $H_0$, so, all of these attributes must be dependent on the edibility, and, more importantly, the edibility is dependent on each of these attributes, meaning using them to train our mahine learning algorithms should work well.\n",
    "\n",
    "## Removing Duplicates\n",
    "We can remove some duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8416, 21)\n",
      "(8124, 21)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "dataset = dataset.drop_duplicates()\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fixed up our dataset, we must now do some Data Transformation\n",
    "# Data Transformation\n",
    "all of our $22$ attributes can be divided into 3 categories; Ordinal, Nominal and Binary Nominal attributes\n",
    "\n",
    "Binary Nominal Attributes with only two values only require one binary flag (0,1) to represent it's data.\n",
    "- №0 'edibility': {'EDIBLE':1,'POISONOUS':0}\n",
    "- №4 'bruises?' : {'BRUISES':1,'NO':0}\n",
    "- №6 'gill-attachment' : {'FREE':0,'ATTACHED':1}\n",
    "- №8 'gill-size' : {'BROAD':0,'NARROW':1}\n",
    "- №10 'stalk-shape' : {'ENLARGING':0,'TAPERING':1}\n",
    "\n",
    "\n",
    "Ordinal attributes can be use a mapping dictionary to map each category to an integer.\n",
    "- №7 'gill-spacing' : {'CLOSE':0,'CROWDED':1,'DISTANT':2}\n",
    "- №18 'ring-number' : {'NONE':0,'ONE':1,'TWO':2}\n",
    "- №21 'population' : {'ABUNDANT':6,'CLUSTERED':5,'NUMEROUS':4,'SCATTERED':3,'SEVERAL':2,'SOLITARY':1}\n",
    "\n",
    "And the rest are Nominal chategorical attributes so will likely have to be one-hot encoded\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Nominal Attributes\n",
    "edibility_map_dict = {\"EDIBLE\":1,\"POISONOUS\":0}\n",
    "bruises_map_dict = {\"BRUISES\":1,\"NO\":0}\n",
    "gill_attachment_map_dict = {\"FREE\":0,\"ATTACHED\":1}\n",
    "gill_size_map_dict = {'BROAD':0,'NARROW':1}\n",
    "stalk_shape_map_dict = {'ENLARGING':0,'TAPERING':1}\n",
    "# Ordinal Attributes\n",
    "gill_spacing_map_dict = {'CLOSE':0,'CROWDED':1,'DISTANT':2}\n",
    "ring_number_map_dict = {'NONE':0,'ONE':1,'TWO':2}\n",
    "population_map_dict = {'ABUNDANT':6,'CLUSTERED':5,'NUMEROUS':4,'SCATTERED':3,'SEVERAL':2,'SOLITARY':1}\n",
    "\n",
    "mapping_dicts = [edibility_map_dict, bruises_map_dict, gill_attachment_map_dict, gill_size_map_dict, stalk_shape_map_dict, gill_spacing_map_dict, ring_number_map_dict, population_map_dict]\n",
    "mapping_columns = ['edibility','bruises?','gill-attachment','gill-size','stalk-shape','gill-spacing','ring-number','population']\n",
    "\n",
    "for i in range(len(mapping_columns)):\n",
    "    dataset[mapping_columns[i]] = dataset[mapping_columns[i]].map(mapping_dicts[i])\n",
    "    \n",
    "onehot_columns = ['cap-shape','cap-surface','cap-color','odor','gill-color','stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring','stalk-color-below-ring','veil-color','ring-type','spore-print-color','habitat']\n",
    "cleanDataset = pd.get_dummies(dataset, columns=onehot_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the dataset is cleaned, and stored in the new Pandas DF `cleanDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8124, 100)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanDataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanDataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edibility</th>\n",
       "      <th>bruises?</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>stalk-shape</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>population</th>\n",
       "      <th>cap-shape_BELL</th>\n",
       "      <th>cap-shape_CONICAL</th>\n",
       "      <th>...</th>\n",
       "      <th>spore-print-color_PURPLE</th>\n",
       "      <th>spore-print-color_WHITE</th>\n",
       "      <th>spore-print-color_YELLOW</th>\n",
       "      <th>habitat_GRASSES</th>\n",
       "      <th>habitat_LEAVES</th>\n",
       "      <th>habitat_MEADOWS</th>\n",
       "      <th>habitat_PATHS</th>\n",
       "      <th>habitat_URBAN</th>\n",
       "      <th>habitat_WASTE</th>\n",
       "      <th>habitat_WOODS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8124.000000</td>\n",
       "      <td>8124.000000</td>\n",
       "      <td>8124.000000</td>\n",
       "      <td>8124.000000</td>\n",
       "      <td>8124.000000</td>\n",
       "      <td>8124.000000</td>\n",
       "      <td>8124.000000</td>\n",
       "      <td>8124.000000</td>\n",
       "      <td>8124.000000</td>\n",
       "      <td>8124.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8124.000000</td>\n",
       "      <td>8124.000000</td>\n",
       "      <td>8124.000000</td>\n",
       "      <td>8124.000000</td>\n",
       "      <td>8124.000000</td>\n",
       "      <td>8124.000000</td>\n",
       "      <td>8124.000000</td>\n",
       "      <td>8124.000000</td>\n",
       "      <td>8124.000000</td>\n",
       "      <td>8124.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.517971</td>\n",
       "      <td>0.415559</td>\n",
       "      <td>0.025849</td>\n",
       "      <td>0.161497</td>\n",
       "      <td>0.309207</td>\n",
       "      <td>0.567208</td>\n",
       "      <td>1.069424</td>\n",
       "      <td>2.355982</td>\n",
       "      <td>0.055638</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.293944</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.264402</td>\n",
       "      <td>0.102413</td>\n",
       "      <td>0.035943</td>\n",
       "      <td>0.140817</td>\n",
       "      <td>0.045298</td>\n",
       "      <td>0.023634</td>\n",
       "      <td>0.387494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499708</td>\n",
       "      <td>0.492848</td>\n",
       "      <td>0.158695</td>\n",
       "      <td>0.368011</td>\n",
       "      <td>0.462195</td>\n",
       "      <td>0.495493</td>\n",
       "      <td>0.271064</td>\n",
       "      <td>1.252082</td>\n",
       "      <td>0.229235</td>\n",
       "      <td>0.022185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076644</td>\n",
       "      <td>0.455595</td>\n",
       "      <td>0.076644</td>\n",
       "      <td>0.441041</td>\n",
       "      <td>0.303209</td>\n",
       "      <td>0.186159</td>\n",
       "      <td>0.347854</td>\n",
       "      <td>0.207969</td>\n",
       "      <td>0.151914</td>\n",
       "      <td>0.487208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         edibility     bruises?  gill-attachment  gill-spacing    gill-size  \\\n",
       "count  8124.000000  8124.000000      8124.000000   8124.000000  8124.000000   \n",
       "mean      0.517971     0.415559         0.025849      0.161497     0.309207   \n",
       "std       0.499708     0.492848         0.158695      0.368011     0.462195   \n",
       "min       0.000000     0.000000         0.000000      0.000000     0.000000   \n",
       "25%       0.000000     0.000000         0.000000      0.000000     0.000000   \n",
       "50%       1.000000     0.000000         0.000000      0.000000     0.000000   \n",
       "75%       1.000000     1.000000         0.000000      0.000000     1.000000   \n",
       "max       1.000000     1.000000         1.000000      1.000000     1.000000   \n",
       "\n",
       "       stalk-shape  ring-number   population  cap-shape_BELL  \\\n",
       "count  8124.000000  8124.000000  8124.000000     8124.000000   \n",
       "mean      0.567208     1.069424     2.355982        0.055638   \n",
       "std       0.495493     0.271064     1.252082        0.229235   \n",
       "min       0.000000     0.000000     1.000000        0.000000   \n",
       "25%       0.000000     1.000000     2.000000        0.000000   \n",
       "50%       1.000000     1.000000     2.000000        0.000000   \n",
       "75%       1.000000     1.000000     3.000000        0.000000   \n",
       "max       1.000000     2.000000     6.000000        1.000000   \n",
       "\n",
       "       cap-shape_CONICAL  ...  spore-print-color_PURPLE  \\\n",
       "count        8124.000000  ...               8124.000000   \n",
       "mean            0.000492  ...                  0.005908   \n",
       "std             0.022185  ...                  0.076644   \n",
       "min             0.000000  ...                  0.000000   \n",
       "25%             0.000000  ...                  0.000000   \n",
       "50%             0.000000  ...                  0.000000   \n",
       "75%             0.000000  ...                  0.000000   \n",
       "max             1.000000  ...                  1.000000   \n",
       "\n",
       "       spore-print-color_WHITE  spore-print-color_YELLOW  habitat_GRASSES  \\\n",
       "count              8124.000000               8124.000000      8124.000000   \n",
       "mean                  0.293944                  0.005908         0.264402   \n",
       "std                   0.455595                  0.076644         0.441041   \n",
       "min                   0.000000                  0.000000         0.000000   \n",
       "25%                   0.000000                  0.000000         0.000000   \n",
       "50%                   0.000000                  0.000000         0.000000   \n",
       "75%                   1.000000                  0.000000         1.000000   \n",
       "max                   1.000000                  1.000000         1.000000   \n",
       "\n",
       "       habitat_LEAVES  habitat_MEADOWS  habitat_PATHS  habitat_URBAN  \\\n",
       "count     8124.000000      8124.000000    8124.000000    8124.000000   \n",
       "mean         0.102413         0.035943       0.140817       0.045298   \n",
       "std          0.303209         0.186159       0.347854       0.207969   \n",
       "min          0.000000         0.000000       0.000000       0.000000   \n",
       "25%          0.000000         0.000000       0.000000       0.000000   \n",
       "50%          0.000000         0.000000       0.000000       0.000000   \n",
       "75%          0.000000         0.000000       0.000000       0.000000   \n",
       "max          1.000000         1.000000       1.000000       1.000000   \n",
       "\n",
       "       habitat_WASTE  habitat_WOODS  \n",
       "count    8124.000000    8124.000000  \n",
       "mean        0.023634       0.387494  \n",
       "std         0.151914       0.487208  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       1.000000  \n",
       "max         1.000000       1.000000  \n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanDataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all our data is clean, encoded and ready to go, lets get started by splitting the data up into all the relevant groups.\n",
    "\n",
    "# Data Splitting\n",
    "\n",
    "If we assume we will be using a simple data splitting method, we simply need to have both the $X$ and $y$ for training aand testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: (8124,)\n",
      "X: (8124, 99)\n"
     ]
    }
   ],
   "source": [
    "y = cleanDataset['edibility'].values\n",
    "big_X = cleanDataset.iloc[:, 1:].values\n",
    "\n",
    "print(\"y: \"+str(y.shape) + \"\\nX: \"+str(big_X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "Do this after you have separated X from y. then we can reduce the number of dimensions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data is scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(big_X)\n",
    "\n",
    "big_X = scaler.transform(big_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09334823 0.16563011 0.23398327 0.2864628  0.33312077 0.37203261\n",
      " 0.40736679 0.43981687 0.46211254 0.4831915  0.50365351 0.52359756\n",
      " 0.54016991 0.55548421 0.57025705 0.58455936 0.59775198 0.61062356\n",
      " 0.62335384 0.6353884  0.64711551 0.65850019 0.66972568 0.68085245\n",
      " 0.69175293 0.70262476 0.71324887 0.72383033 0.73416897 0.74439077\n",
      " 0.75455182 0.76471286 0.77483381 0.78494058 0.79473966 0.80435809\n",
      " 0.81375071 0.82301616 0.83177261 0.84050109 0.84880625 0.8570539\n",
      " 0.86516985 0.87296689 0.88054877 0.88779901 0.89480372 0.90158735\n",
      " 0.90827664 0.9148437  0.9210266  0.92704299 0.93275899 0.93810232\n",
      " 0.94334334 0.94846095 0.95332733 0.95797713 0.96212289 0.96620448\n",
      " 0.9699663  0.97354174 0.97694108 0.98023273 0.98325091 0.985901\n",
      " 0.98851784 0.99067532 0.99278709 0.99458699 0.99635576 0.99780134\n",
      " 0.99875104 0.99919766 0.99955193 0.99977648 0.99988267 0.99997843\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.        ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "temp_X = pca.fit_transform(big_X)\n",
    "\n",
    "variances = pca.explained_variance_ratio_\n",
    "cum_sum_variances = np.cumsum(variances)\n",
    "print(cum_sum_variances)\n",
    "\n",
    "plt.bar(range(0,99), variances, alpha = 0.5, align = 'center', label='Individual variance')\n",
    "plt.step(range(0,99), cum_sum_variances, where = 'mid', label = 'Cumulative variance')\n",
    "plt.ylabel('Explained variance')\n",
    "plt.xlabel('Attributes index')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Graph showing Explained Variance of attributes')\n",
    "\n",
    "\n",
    "variance_plot = sns.lineplot(data = variances, markers = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8124, 10)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=10, random_state=1)\n",
    "X = pca.fit_transform(big_X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the columns in their correct places and the dimensions of X reduced, lets shuffle them around, and then split them into the correct Test and Train sets.\n",
    "Note that the ratio of columns assigned to the train set and the test set can be changed by changing the variable `train_ratio` \n",
    "\n",
    "We split the data using a stratified splitter into train and tests. But have also shuffled the entire X,y dataset Dataframes. This is so that they can be used for cross validation.\n",
    "\n",
    "We have the version of the splitter to **stratify** the random splitting. This way the proportions of examples in both classes for y is maintained in both splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5686, 10) (5686,)\n",
      "(2438, 10) (2438,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#test_ratio = 0.2\n",
    "test_ratio = 0.3\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_ratio, random_state = 0, stratify = y)\n",
    "\n",
    "indices = np.arange(cleanDataset.shape[0])\n",
    "rng = np.random.RandomState(0)\n",
    "permuted_indices = rng.permutation(indices)\n",
    "X = X[permuted_indices]\n",
    "y = y[permuted_indices]\n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1175, 1: 1263}\n",
      "{0: 2741, 1: 2945}\n"
     ]
    }
   ],
   "source": [
    "def Counts(ArrayToCount):\n",
    "    vals, count = np.unique(ArrayToCount, return_counts=True)\n",
    "    return (dict(zip(vals, count)))\n",
    "\n",
    "print(Counts(y_test))\n",
    "print(Counts(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have split the data into all the seperate parts. \n",
    "- `X_train`\n",
    "- `X_test`\n",
    "- `y_train`\n",
    "- `y_test`\n",
    "\n",
    "**note** that these can now be used for training and testing, although if any alterations happen during the algorithms themselves, be sure to copy these into a local variable so that in the end we are able to run all of the algorithms with one simple click."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy for Training Set is 94.03798804080196\n",
      "The Accuracy for Test Set is 94.298605414274\n",
      "The Precision for Test Set is 93.16436251920123\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94      1175\n",
      "           1       0.93      0.96      0.95      1263\n",
      "\n",
      "    accuracy                           0.94      2438\n",
      "   macro avg       0.94      0.94      0.94      2438\n",
      "weighted avg       0.94      0.94      0.94      2438\n",
      "\n",
      "Cross-Validation scores: [0.94341943 0.94464945 0.94464945 0.93849938 0.9273399  0.94581281\n",
      " 0.94704433 0.95320197 0.94827586 0.93596059]\n",
      "Average score: 0.9428853180157418\n"
     ]
    }
   ],
   "source": [
    "LogReg = LogisticRegression(solver='lbfgs', max_iter=1000) \n",
    "LogReg.fit(X_train, y_train)\n",
    "y_pred=LogReg.predict(X_test)\n",
    "train_acc = LogReg.score(X_train, y_train) \n",
    "print(\"The Accuracy for Training Set is {}\".format(train_acc*100)) \n",
    "test_acc = accuracy_score(y_test, y_pred) \n",
    "print(\"The Accuracy for Test Set is {}\".format(test_acc*100)) \n",
    "print(\"The Precision for Test Set is {}\".format(precision_score(y_test, y_pred)*100))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "scores = cross_val_score(LogReg, X, y, cv=10)\n",
    "\n",
    "print('Cross-Validation scores: {}'.format(scores))\n",
    "print('Average score: {}'.format(np.mean(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0, 'penalty': 'l1', 'solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "#grid search\n",
    "C = np.logspace(0, 4, num=10)\n",
    "penalty = ['l1', 'l2']\n",
    "solver = ['liblinear', 'saga']\n",
    "hyperparameters = dict(C=C, penalty=penalty, solver=solver)\n",
    "logistic =LogisticRegression()\n",
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=10)\n",
    "best_model_grid = gridsearch.fit(X_train,y_train)\n",
    "print(best_model_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation scores: [0.94218942 0.94464945 0.94464945 0.93849938 0.9273399  0.94704433\n",
      " 0.94704433 0.95320197 0.94827586 0.93472906]\n",
      "Average score: 0.9427623167857295\n"
     ]
    }
   ],
   "source": [
    "LogReg2=LogisticRegression(C=1,penalty=\"l1\",solver='saga')\n",
    "LogReg2.fit(X_train,y_train)\n",
    "\n",
    "scores = cross_val_score(LogReg2, X, y, cv=10)\n",
    "\n",
    "print('Cross-Validation scores: {}'.format(scores))\n",
    "print('Average score: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=464.15888336127773, penalty='l1', solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "#random search\n",
    "C = np.logspace(0, 4, num=10)\n",
    "penalty = ['l1', 'l2']\n",
    "solver = ['liblinear', 'saga']\n",
    "hyperparameters = dict(C=C, penalty=penalty, solver=solver)\n",
    "logistic =LogisticRegression()\n",
    "\n",
    "randomizedsearch = RandomizedSearchCV(logistic, hyperparameters)\n",
    "best_model_random = randomizedsearch.fit(X_train,y_train)\n",
    "print(best_model_random.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation scores: [0.94341943 0.94464945 0.94464945 0.93849938 0.9273399  0.94704433\n",
      " 0.94704433 0.95320197 0.94827586 0.93596059]\n",
      "Average score: 0.9430084707251014\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94      3916\n",
      "           1       0.93      0.96      0.95      4208\n",
      "\n",
      "    accuracy                           0.94      8124\n",
      "   macro avg       0.94      0.94      0.94      8124\n",
      "weighted avg       0.94      0.94      0.94      8124\n",
      "\n",
      "[[3617  299]\n",
      " [ 164 4044]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "LogReg3=LogisticRegression(C=1, penalty='l2', solver='liblinear')\n",
    "\n",
    "# LogReg3.fit(X_train,y_train)\n",
    "\n",
    "scores = cross_val_score(LogReg3, X, y, cv=10)\n",
    "\n",
    "print('Cross-Validation scores: {}'.format(scores))\n",
    "logregBestScore = np.mean(scores)\n",
    "print('Average score: {}'.format(logregBestScore))\n",
    "\n",
    "\n",
    "\n",
    "LogRegPredictions = cross_val_predict(LogReg3, X, y, cv=10)\n",
    "print(classification_report(y, LogRegPredictions))\n",
    "\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(y, LogRegPredictions)\n",
    "print(conf_matrix)\n",
    "\n",
    "\n",
    "\n",
    "plt.matshow(conf_matrix, cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nearest callable function\n",
    "neighbors default is 5, so this is a reasonable value to pass in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 1 1 1]\n",
      "The Accuracy for Test Set is 100.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1175\n",
      "           1       1.00      1.00      1.00      1263\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def knearest(X_train, y_train, X_test, neighbors, distance_metric):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    kNeighbors = KNeighborsClassifier(n_neighbors=neighbors, metric=distance_metric)\n",
    "    kNeighbors.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = kNeighbors.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "\n",
    "knearest_pred = knearest(X_train, y_train, X_test, 2, 'euclidean')\n",
    "# {'n_neighbors':{1,2,3,4,5,6,7,8,9,10},'metric':{'euclidean','manhattan','minkowski','chebyshev','wminkowski','seuclidean','mahalanobis'}, 'algorithm':{'ball_tree','kd_tree','brute','auto'}}\n",
    "\n",
    "print (knearest_pred)\n",
    "\n",
    "test_acc_k_nearest = accuracy_score(y_test, knearest_pred) \n",
    "print(\"The Accuracy for Test Set is {}\".format(test_acc_k_nearest*100)) \n",
    "\n",
    "# Generate classification report \n",
    "print(classification_report(y_test, knearest_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now attempt to do some hyper parameter tuning to find the best algirhtms and whatnots to perform the algorithm with. then also do a cross validation using these to get a good result indicative of the accuracy of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    9.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metric': 'euclidean', 'n_neighbors': 1}\n",
      "1.0\n",
      "Cross-Validation scores: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Average score: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3916\n",
      "           1       1.00      1.00      1.00      4208\n",
      "\n",
      "    accuracy                           1.00      8124\n",
      "   macro avg       1.00      1.00      1.00      8124\n",
      "weighted avg       1.00      1.00      1.00      8124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knnModel = KNeighborsClassifier()\n",
    "knnHyperParams = {'n_neighbors':(1,2,3,4,5,6,7,8,9,10),'metric':('euclidean','manhattan','minkowski','chebyshev')}\n",
    "grid = GridSearchCV(knnModel, knnHyperParams, cv=10, scoring='accuracy',  return_train_score=False,verbose=1)\n",
    "grid_search = grid.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "\n",
    "\n",
    "bestKnn = KNeighborsClassifier(n_neighbors = grid_search.best_params_['n_neighbors'], metric = grid_search.best_params_['metric'])\n",
    "knnScores = cross_val_score(bestKnn, X, y, cv=10)\n",
    "\n",
    "print('Cross-Validation scores: {}'.format(knnScores))\n",
    "knnBestScore = (np.mean(knnScores))\n",
    "print('Average score: {}'.format(knnBestScore))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "knnPredictions = cross_val_predict(bestKnn, X, y, cv=10)\n",
    "print(classification_report(y, knnPredictions))\n",
    "\n",
    "#eval_confusion(bestKnn, X, y, knnPredictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masha's section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_func(X_train, y_train, X_test, y_test):\n",
    "    #Libraries used : \n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    ###\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('clf', DecisionTreeClassifier(criterion = 'entropy'))\n",
    "    ])\n",
    "\n",
    "    #Limiting the depth of the tree to be used to prevent overfitting.\n",
    "    ######With [clf__max_depth': (5, 6)] we aleady get 1.00 for f1 score.\n",
    "    ######f1 starts going below 1.00 at ['clf__max_depth': (4, 5)]. At this setting f-1 is 0.99\n",
    "    parameters = {\n",
    "        'clf__max_depth': (5, 15, 20, 25, 30),\n",
    "        'clf__min_samples_split': (2, 3, 4),\n",
    "        'clf__min_samples_leaf': (1, 2, 3, 4)\n",
    "    }\n",
    "\n",
    "    # parameters = {\n",
    "    #     'clf__max_depth': (200, 300),\n",
    "    #     'clf__min_samples_split': (5, 6),\n",
    "    #     'clf__min_samples_leaf': (5, 6)\n",
    "    # }\n",
    "\n",
    "    #Creating the machine learning model with decision trees using GridSearchCV\n",
    "    grid_search_model = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='f1')\n",
    "    grid_search_model.fit(X_train, y_train)\n",
    "    \n",
    "    print('Best score: %0.3f' % grid_search_model.best_score_)\n",
    "    print('Best parameters set:')\n",
    "    best_parameters = grid_search_model.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print('t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "    decision_tree_predictions = grid_search_model.predict(X_test)\n",
    "\n",
    "    #Priting model evaluation based on its f1 score.\n",
    "    print(classification_report(y_test, decision_tree_predictions))\n",
    "    #Returning the model and its prediction. To be used in performance analysis.\n",
    "    return grid_search_model, decision_tree_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up X and y with the original dataset.\n",
    "To be used in comparison of machine learning model performance over original data and clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "#(for OneHotEncoding)\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#Creating a dataframe called \"df\" using the data from mushrooms.csv\n",
    "df = pd.read_csv('./mush.csv')\n",
    "\n",
    "#Creating a list of all column_names from the 1st row in the dataset.\n",
    "#Used in OneHotEncoding\n",
    "column_names = \"edibility,cap-shape,cap-surface,cap-color,bruises?,odor,gill-attachment,gill-spacing,gill-size,gill-color,stalk-shape,stalk-root,stalk-surface-above-ring,stalk-surface-below-ring,stalk-color-above-ring,stalk-color-below-ring,veil-type,veil-color,ring-number,ring-type,spore-print-color,population,habitat\"\n",
    "column_names_list = column_names.split(\",\")\n",
    "\n",
    "#Creating a new dataframe X by copying the original dataframe into it.\n",
    "X_orig = df.copy()\n",
    "#Removing the answer column from the input dataframe X.\n",
    "#This is to prevent the model from learning the answers.\n",
    "del X_orig[\"edibility\"]\n",
    "\n",
    "#For future check - to see if OneHotEncoding has worked.\n",
    "#print(\"Old size of X: \", X.shape)\n",
    "\n",
    "#Generating a 2D array called unique_variable_values.\n",
    "#It will contain arrays in such way, that each array with index i has all unique variable values of column i in the dataset (for 0 <= i < number of columns).\n",
    "unique_variable_values = []\n",
    "first = True\n",
    "for col in df:\n",
    "    if first:\n",
    "        first = False\n",
    "        continue\n",
    "    unique_variable_values.append(list(df[col].unique()))    \n",
    "#To check if unique_variable_values array was generated correctly\n",
    "#print(unique_variable_values)\n",
    "\n",
    "#Using OneHotEncoder on X, creating a representation of our mushroom data set (contains nominal data only) that can be used in machine learning.\n",
    "ohe = OneHotEncoder(categories = unique_variable_values)\n",
    "X_new_orig = ohe.fit_transform(X_orig).toarray()\n",
    "#To check if OneHotEncoding has worked.\n",
    "#print(\"New X shape: \", X_new.shape)\n",
    "\n",
    "#Setting up y to contain the answer column. In our case each row will either say \"Edible\" or \"Poisonous\"\n",
    "y_orig = df[\"edibility\"]\n",
    "\n",
    "#Performing a test-train split on y and X.\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X_new_orig, y_orig, shuffle=True)\n",
    "#Setting shuffle to True to make sure the data is split randomly.\n",
    "#This prevents unintentional bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pos_label=1 is not a valid label: array(['EDIBLE', 'POISONOUS'], dtype='<U9')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/rinajeong/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"/Users/rinajeong/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/Users/rinajeong/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/rinajeong/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 252, in __call__\n    return [func(*args, **kwargs)\n  File \"/Users/rinajeong/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 252, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/Users/rinajeong/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 560, in _fit_and_score\n    test_scores = _score(estimator, X_test, y_test, scorer)\n  File \"/Users/rinajeong/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 607, in _score\n    scores = scorer(estimator, X_test, y_test)\n  File \"/Users/rinajeong/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 87, in __call__\n    score = scorer._score(cached_call, estimator,\n  File \"/Users/rinajeong/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 212, in _score\n    return self._sign * self._score_func(y_true, y_pred,\n  File \"/Users/rinajeong/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n    return f(**kwargs)\n  File \"/Users/rinajeong/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\", line 1044, in f1_score\n    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n  File \"/Users/rinajeong/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n    return f(**kwargs)\n  File \"/Users/rinajeong/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\", line 1168, in fbeta_score\n    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n  File \"/Users/rinajeong/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n    return f(**kwargs)\n  File \"/Users/rinajeong/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\", line 1433, in precision_recall_fscore_support\n    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n  File \"/Users/rinajeong/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\", line 1256, in _check_set_wise_labels\n    raise ValueError(\"pos_label=%r is not a valid label: \"\nValueError: pos_label=1 is not a valid label: array(['EDIBLE', 'POISONOUS'], dtype='<U9')\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-066c7291e868>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Using the decision tree model on original data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdecision_tree_model_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision_tree_predictions_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-120-8185f8061692>\u001b[0m in \u001b[0;36mdecision_tree_func\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#Creating the machine learning model with decision trees using GridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mgrid_search_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mgrid_search_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best score: %0.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgrid_search_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: pos_label=1 is not a valid label: array(['EDIBLE', 'POISONOUS'], dtype='<U9')"
     ]
    }
   ],
   "source": [
    "#Using the decision tree model on original data\n",
    "decision_tree_model_orig, decision_tree_predictions_orig = decision_tree_func(X_train_orig, y_train_orig, X_test_orig, y_test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_confusion(decision_tree_model_orig, X_test_orig, y_test_orig, decision_tree_predictions_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the decision tree model on clean data\n",
    "decision_tree_model, decision_tree_predictions = decision_tree_func(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_confusion(decision_tree_model, X_test, y_test, decision_tree_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_func(X_train, y_train, X_test, y_test):\n",
    "    #Libraries used :\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('clf', RandomForestClassifier(criterion = 'entropy'))\n",
    "    ])\n",
    "\n",
    "    #Limiting the depth of the tree to be used to prevent overfitting.\n",
    "    #While experimenting with decison_tree_procedure(), I have already found that [clf__max_depth': (5, 6)] is optimal for our dataset.\n",
    "    #This remains true for the random forest model as well. 5 is still the optimal depth - gives us no overfitting and no underfitting.\n",
    "    \n",
    "    #With 10 estimators we already get f1 in the range of 0.99 and 1.00. To reduce the computation time, I checked f1 with different enstimators used.\n",
    "    #Having 9 estimators results in f1 = [0.98, 0.99]\n",
    "    #Setting clf__n_estimators to 5 gives an average f1 of 0.97.\n",
    "    #Increasing the the number of estimators to 11 also gave f1 betwee 0.99 and 1.00. However, in this case the 1.00 could be seen more often, compared to the use of 10 estimators.\n",
    "    #Therefore, it is best to use 11 estimators with original data - to not harm the accuracy and reduce computational resourses.\n",
    "    parameters = {\n",
    "        #'clf__max_depth': (3, 4, 5),\n",
    "        'clf__n_estimators': (11, 15, 20),\n",
    "        # 'clf__max_depth': (65, 6),\n",
    "        'clf__max_depth': (6, 7),\n",
    "        'clf__min_samples_split': (2, 3),\n",
    "        'clf__min_samples_leaf': (1, 2, 3)\n",
    "    }\n",
    "\n",
    "    #Creating the machine learning model with decision trees using GridSearchCV\n",
    "    random_forest_model = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='f1')\n",
    "    random_forest_model.fit(X_train, y_train)\n",
    "    best_parameters = random_forest_model.best_estimator_.get_params()\n",
    "    print('Best score: %0.3f' % random_forest_model.best_score_)\n",
    "    print('Best parameters set:')\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print('t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "    random_forest_predictions = random_forest_model.predict(X_test)\n",
    "\n",
    "    #Priting model evaluation based on its f1 score.\n",
    "    print(classification_report(y_test, random_forest_predictions))\n",
    "    return random_forest_model, random_forest_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the random forest model on original data\n",
    "random_forest_model_orig, random_forest_predictions_orig = random_forest_func(X_train_orig, y_train_orig, X_test_orig, y_test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_confusion(random_forest_model_orig, X_test_orig, y_test_orig, random_forest_predictions_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest on Clean version of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the random forest model on clean data\n",
    "random_forest_model, random_forest_predictions = random_forest_func(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_confusion(random_forest_model, X_test, y_test, random_forest_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [knnBestScore, logregBestScore]\n",
    "print(results)\n",
    "\n",
    "df = pd.DataFrame({'labels':['knn', 'logreg'], 'scores':results})\n",
    "\n",
    "print(df.T)\n",
    "\n",
    "sns.barplot(x = df['labels'], y = df['scores'])\n",
    "plt.ylim((0.9,1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
